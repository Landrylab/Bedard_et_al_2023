{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informations\n",
    "\n",
    "To use this script, you have to start with .needle files.\n",
    "It's important to use the same sample sheet than part 1. You'll also need the reads_count excel sheet to add more informations to it.\n",
    "I made the script so you don't have to change anything except in the set up section.\n",
    "\n",
    "## This is the part 2 of the analysis. \n",
    "\n",
    "### Analysis pipeline : \n",
    "### 1- Parse_needle_output() from .needle files\n",
    "### 2- Find_mutations() from the parse_needle_output()\n",
    "### 3- get_variant_count_1() from Find_mutations() output.\n",
    "### 4- Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'm running this code locally on windows (don't need to be using the subsystem for that) or on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import Bio\n",
    "import subprocess\n",
    "import openpyxl\n",
    "from Bio import SeqIO\n",
    "import os,sys,re\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.__name__, pd.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__name__, np.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "print(matplotlib.__name__, matplotlib.__version__)\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "print(scipy.__name__, scipy.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "print(sns.__name__, sns.__version__)\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "Ideally this is the only cell which has to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##see the sample sheet to know how to fill it\n",
    "##info = dataframe of the sample_sheet\n",
    "## wt = the path to the wt sequence in a .fa file\n",
    "## wt_seq = wt sequence directly in a string\n",
    "## *the wt sequence has to be without the primers, so how it will be once trimmed.\n",
    "\n",
    "date = \"2023-06-20\" #date your doing the analysis, used to save figures\n",
    "experiment = \"CaERG11-F3-Part2\" #generic name of your experiment, used to save figures\n",
    "reads_folder = \"./CaERG11-F3/\" #location of the reads in .gz\n",
    "merged_folder = \"./CaERG11-F3-merged_reads/\" # location of merged reads\n",
    "figures_folder = \"./CaERG11-F3-figures/\" # location of the figures\n",
    "trim_folder = \"./CaERG11-F3-Trimmed/\" #location of trimmed reads\n",
    "agg_folder = \"./CaERG11-F3-Aggregated/\" #location of aggregated reads\n",
    "needle_folder = \"./CaERG11-F3-Needle/\" #location of reads aligned to wt (.needle)\n",
    "variants_folder = \"./CaERG11-F3-variants/\" #location of the variants count\n",
    "reads_counts_folder = \"./CaERG11-F3-ReadsCount/\" #location of all dataframes output in excel format with reads counts \n",
    "heatmaps_folder = \"./CaERG11-F3-Heatmaps/\" #location of all heatmaps\n",
    "\n",
    "sample_sheet = \"./sample_sheet_F3_all.xlsx\" #excel sheet with all the information\n",
    "reads_count = reads_counts_folder + \"Reads_count_CaERG11-F3_2023-06-20.xlsx\" \n",
    "\n",
    "pe = 250 #paired-end\n",
    "\n",
    "before_nut = 2 #number of nucleotides before the first complete codon of your sequence (0, 1 or 2)\n",
    "after_nut = 2 #number of nucleotides after the last complete codon of your sequence (0, 1 or 2)\n",
    "\n",
    "aa_start = 276 #first amino acid of your sequence (complete codon)\n",
    "aa_end = 406 #last amino acid of your sequence (complete codon)\n",
    "\n",
    "wt = \"./CaERG11_wt_seq/CaERG11_F3_wt_seq.fasta\" #your WT sequence after trimming\n",
    "wt_seq = \"TTGATTGATTCTCTGCTGATTCATAGCACATACAAAGACGGTGTGAAGATGACCGACCAGGAGATTGCAAACCTGCTTATAGGTATTTTAATGGGTGGACAGCACACATCAGCTTCTACGTCAGCATGGTTTCTGCTACACCTGGGGGAAAAACCGCACCTACAAGACGTAATATATCAAGAGGTGGTGGAGTTACTAAAAGAGAAAGGAGGCGATCTAAACGATTTAACCTATGAGGATCTGCAAAAATTGCCGTCAGTCAACAACACGATCAAGGAAACGCTAAGGATGCACATGCCTCTACACAGTATATTCAGAAAAGTTACTAACCCACTTAGGATCCCTGAAACCAACTACATCGTCCCAAAAGGACACTACGTTCTTGTCAGCCCA\"\n",
    "pos_mutated = [276,277,280,292,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,362,363,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,401,402,403,404,405,406]\n",
    "codons_mutated = [\"GCT\",\"GCC\",\"TGC\",\"TGT\",\"GAT\",\"GAC\",\"GAG\",\"GAA\",\n",
    "\"TTC\",\"TTT\",\"GGT\",\"GGA\",\"CAC\",\"CAT\",\"ATC\",\"ATT\",\"AAG\",\"AAA\",\"TTG\",\"TTA\",\"ATG\",\"AAC\",\"AAT\",\"CCT\",\"CCA\",\"CAG\",\"CAA\",\n",
    "\"AGA\",\"CGT\",\"TCT\",\"TCC\",\"ACC\",\"ACT\",\"GTT\",\"GTC\",\"TGG\",\"TAC\",\"TAT\",\"TAA\"] \n",
    "codons_all = [\"GCT\",\"GCC\",\"TGC\",\"TGT\",\"GAT\",\"GAC\",\"GAG\",\"GAA\",\n",
    "\"TTC\",\"TTT\",\"GGT\",\"GGA\",\"CAC\",\"CAT\",\"ATC\",\"ATT\",\"AAG\",\"AAA\",\"TTG\",\"TTA\",\"ATG\",\"AAC\",\"AAT\",\"CCT\",\"CCA\",\"CAG\",\"CAA\",\n",
    "\"AGA\",\"CGT\",\"TCT\",\"TCC\",\"ACC\",\"ACT\",\"GTT\",\"GTC\",\"TGG\",\"TAC\",\"TAT\",\"TAA\",\"GTA\",\"GCA\",\"GTG\",\"ATA\",\"GCG\",\"CTC\",\"CTA\",\"CGG\",\n",
    "\"TAG\",\"TGA\",\"CCC\",\"GGG\",\"TCG\",\"AGG\",\"CGA\",\"CGC\",\"AGT\",\"CTG\",\"ACG\",\"TCA\",\"AGC\",\"GGC\",\"CTT\",\"ACA\",\"CCG\"]\n",
    "wt_len = len(wt_seq)\n",
    "wt_aa=\"LIDSLLIHSTYKDGVKMTDQEIANLLIGILMGGQHTSASTSAWFLLHLGEKPHLQDVIYQEVVELLKEKGGDLNDLTYEDLQKLPSVNNTIKETLRMHMPLHSIFRKVTNPLRIPETNYIVPKGHYVLVSP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_excel(sample_sheet, header=0, index_col=0)\n",
    "reads = pd.read_excel(reads_count, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the needle output to get variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a bunch of function and then we use them all at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_needle_output(path):\n",
    "    # this function parses the needle alignments and extract the aligned sequences reference and query seuqences. \n",
    "    # It takes as input the path to the needle output\n",
    "    \n",
    "    n_aligns = 0\n",
    "    # counter for the number of alignments\n",
    "    \n",
    "    align_seqs_dict = {}\n",
    "    # empty container that will hold the aligned sequences\n",
    "                \n",
    "    needle_align_path = path\n",
    "    # path to the alignments\n",
    "        \n",
    "    with open(needle_align_path, 'r') as source:\n",
    "        # open the alignment\n",
    "\n",
    "        current_align = ''\n",
    "        current_qseq = ''\n",
    "        current_sseq = ''\n",
    "        # empty container objects for data processing\n",
    "        \n",
    "        qseq_done = 0\n",
    "        #counter for the number of alignments processed\n",
    "\n",
    "        for line in source:\n",
    "            # loop through the file\n",
    "\n",
    "            if line.startswith('>>>') == True:\n",
    "                # detect headers\n",
    "\n",
    "                n_aligns +=1\n",
    "                # increment alignment counter by one\n",
    "\n",
    "                align_name = line.strip('>>>')\n",
    "                # get alignment name\n",
    "\n",
    "                if n_aligns != 1:\n",
    "                    # if this is not the first alignment\n",
    "\n",
    "                    align_seqs_dict[current_align] = [current_qseq, current_sseq]\n",
    "                    # add the information on the previous alignment to the dict\n",
    "\n",
    "                    current_align = align_name\n",
    "                    # update the name for the new entry\n",
    "\n",
    "                    current_qseq = ''\n",
    "                    current_sseq = ''\n",
    "                    # reset temporary variables\n",
    "\n",
    "                    qseq_done = 0\n",
    "                    # reset indicator for query sequence extraction\n",
    "\n",
    "                else:\n",
    "                    current_align = align_name\n",
    "                    # for the fisrt sequence, just need to store the align name\n",
    "\n",
    " \n",
    "            elif line.startswith(';') == False and line.startswith('>') == False and line.startswith('\\n') == False and line.startswith('#') == False:\n",
    "                # skip all the useless lines to process only the aligned sequences\n",
    "\n",
    "                if qseq_done == 1:\n",
    "                    current_sseq += line.strip('\\n')\n",
    "                    # if the query seq is done (qseq = 1), add sequence to the subject\n",
    "\n",
    "                else:\n",
    "                    current_qseq += line.strip('\\n')\n",
    "                    # if the query seq is not done, continue to update it\n",
    "\n",
    "            elif line.startswith('#--') == True:\n",
    "                align_seqs_dict[align_name] = [current_qseq, current_sseq]\n",
    "                # update dict with info from the last entry in the alignment sequence\n",
    "\n",
    "            else:\n",
    "                if qseq_done == 0 and current_qseq != '':\n",
    "                    qseq_done =1\n",
    "                    # if the qseq is recorded, update value\n",
    "                \n",
    "                    \n",
    "    return align_seqs_dict, n_aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mutations(path,ref_orf):\n",
    "    # For a given path and reference sequence, runs the Needle alignment parser and\n",
    "    # then goes through the extracted alignments to find the differences between the aligned query and the reference.\n",
    "    #\n",
    "    \n",
    "    allele_dict = {}\n",
    "    # empty dict that will hold the sequence name :mutations information\n",
    "    \n",
    "    # here you associate the sample name in the sample_dataframe (frag) to the name in the fasta files (ref_orf)\n",
    "    ref_orf = ref_orf\n",
    "    # depending on the sample, select the approptiate reference sequence #give file name\n",
    "    \n",
    "    align_dict, align_count = parse_needle_output(path)\n",
    "    # get the parsed alignments for the sample\n",
    "    \n",
    "    for entry in list(align_dict.keys()):\n",
    "        # loop through aligned sequences\n",
    "\n",
    "        read_var_list = []\n",
    "        # temporary holder for mutations found in sequence\n",
    "\n",
    "        query_seq = align_dict[entry][1]\n",
    "        # aligned cds sequence of the strain\n",
    "\n",
    "        align_ref = align_dict[entry][0]\n",
    "        # aligned cds sequence of the reference\n",
    "\n",
    "        gap_adjust = 0\n",
    "        # value used to adjust the cds sequence index for the presence of insertions in the strain sequence vs the \n",
    "        # reference cds\n",
    "\n",
    "        backtrack_adjust = 0\n",
    "        # value used to adjust the cds sequence index for the presence of deletions in the strain sequence vs the \n",
    "        # reference cds\n",
    "\n",
    "        temp_var = None\n",
    "        # temporary variable to hold the sequence of an insertion or deletion as a string. When the gap ends, annotation \n",
    "        # will be added to read_var_list\n",
    "\n",
    "        indel_start = 0\n",
    "        # position start of the indel annotation in the reference sequence, with adjustment for gap presence\n",
    "\n",
    "        ref_seq_no_gaps = align_ref.replace('-','')\n",
    "        # reference sequence with gaps removed\n",
    "\n",
    "        align_start = 0\n",
    "        # position in the reference fragment where the alignment starts\n",
    "\n",
    "        query_seq_no_gaps = len(query_seq.replace('-',''))\n",
    "        # length of the query when gaps are removed\n",
    "\n",
    "        for nt in range(0, len(align_ref)):\n",
    "            # iterates through the entire alignment of the strain prot sequence\n",
    "\n",
    "            if query_seq[nt] == '-':\n",
    "                # detect a deletion variant\n",
    "\n",
    "                # logic for indel detection/annotation:\n",
    "                #\n",
    "                # suppose we have this alignment  \n",
    "                #\n",
    "                # 1 2 3 4 5 6 7 8 9\n",
    "                # A T - - A A A T G    strain variant: del gaps are indexed because the aa index is based on reference\n",
    "                # A T K P A - - T G\n",
    "                # 1 2 3 4 5     6 7    reference: insert gaps not indexed because aa positions do exist in reference\n",
    "                #\n",
    "                # following this logic, every time an insertion is detected and annotated, the gap_adjust value is \n",
    "                # incremented by the length of the gap and used to adjust the variant mapping to make it match the \n",
    "                # reference index values. The indel aa postion is the first residue detected as part of the indel\n",
    "\n",
    "\n",
    "                if indel_start == 0:\n",
    "                    # checks if the character is the start or the continuation of a gap in the alignment\n",
    "\n",
    "                    temp_var = 'del'+ align_ref[nt]\n",
    "                    indel_start = (nt+1-gap_adjust)\n",
    "                    # if it is, starts a new annotation entry with a start position compensated for previous insertions\n",
    "                    # (if any)\n",
    "\n",
    "                    backtrack_adjust += 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    temp_var += align_ref[nt]\n",
    "                    # if it is not, adds the following aa to the deletion annotation\n",
    "\n",
    "                    backtrack_adjust += 1\n",
    "\n",
    "\n",
    "            elif align_ref[nt] == '-':\n",
    "                # detects an insertion variant\n",
    "\n",
    "                if indel_start == 0:\n",
    "                    # checks if the character is the start or the continuation of a gap in the alignment\n",
    "\n",
    "                    temp_var = 'ins'+ query_seq[nt]\n",
    "\n",
    "                    indel_start = (nt+1-gap_adjust)\n",
    "                    # if it is, starts a new annotation entry with a start position compensated for previous insertions\n",
    "                    # (if any)\n",
    "\n",
    "                    gap_adjust += 1\n",
    "                    # increments the gap adjust for the this added aa in the strain sequence                   \n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    temp_var += query_seq[nt]\n",
    "                    # if it is not, adds the following aa to the insertion annotation\n",
    "\n",
    "                    gap_adjust += 1\n",
    "                    # increments the gap adjust for the this added aa in the strain sequence\n",
    "\n",
    "\n",
    "            elif query_seq[nt] != align_ref[nt]:\n",
    "                # detects a mismatch between the strain sequence and the reference\n",
    "\n",
    "\n",
    "                variant = align_ref[nt]+'|'+str((nt+1-gap_adjust))+'|'+query_seq[nt]\n",
    "                read_var_list.append(variant)\n",
    "                # creates an annotation for the strain-reference aa mismatch and appends it to the list of \n",
    "                # annotations\n",
    "\n",
    "            else:\n",
    "\n",
    "                 if indel_start != 0:\n",
    "                    # detects if there is currently an open gap entry. If there is, then the detected mismatch means \n",
    "                    # that it has now concluded\n",
    "\n",
    "                    read_var_list.append(str((indel_start))+temp_var)\n",
    "                    temp_var = None\n",
    "                    indel_start = 0\n",
    "                    # adds the indel annotation to the strain variant list and resets temporary variables for the next \n",
    "                    # indel entry\n",
    "\n",
    "\n",
    "        if query_seq_no_gaps >=  len(ref_seq_no_gaps)*0.8 and len(read_var_list)<25:             \n",
    "            allele_dict[entry] = read_var_list, align_start\n",
    "            # apply a filter for alignment quality: the alignment must cover at least 80% of the reference sequence and\n",
    "            # there must be less than 3 differences between the query and the reference (insertions, deletions or SNVs)\n",
    "            #not be bigger tha the size of the reference\n",
    "                           \n",
    "    return allele_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_align_dict = {}\n",
    "\n",
    "def get_n_align(allele_dict):\n",
    "    # for a given dict (sequence: mutations), add up the number of\n",
    "    # aggregated sequences for each entry to get the total number of \n",
    "    # reads that passed the alignment filter\n",
    "    \n",
    "    n_align = 0\n",
    "    # counter\n",
    "    \n",
    "    for seq in list(allele_dict.keys()):\n",
    "        # loop through entries in the dict\n",
    "        \n",
    "        var_info = seq.split(',')\n",
    "        var_count =int(var_info[1].split(';')[1].strip('size='))\n",
    "        # get the number of reads for a specific sequence\n",
    "        \n",
    "        n_align += var_count\n",
    "        # add it to the count\n",
    "        after_align_dict[str(info.loc[Sample][\"Name\"])] = n_align\n",
    "    return n_align\n",
    "    # return the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variant_count_1(mutation_set, ref_seq, frag_start, codon_start, n_aa):\n",
    "    \n",
    "    variant_abundance_dict ={}\n",
    "    # container that will store the abundance of protein variants\n",
    "    \n",
    "    variants = list(mutation_set.keys())\n",
    "    # list of detected variants from the find_mutations output\n",
    "    \n",
    "    codon_groups = {}\n",
    "    # This dict will hold the positions in the reference cds that correspond to the same codon.\n",
    "    # This allows for easy verification that detected mutations occur within the same codon later \n",
    "    # downstream in the function\n",
    "    \n",
    "    codon = codon_start-1  #important needs to stay like this\n",
    "    # counter for codon position in the fragment\n",
    "    \n",
    "    wt_count =0\n",
    "    valid_seq=0\n",
    "    # counters for the number of WT cds detected and the number sequences passing the quality filter\n",
    "    \n",
    "    for nt in range(0, n_aa*3):\n",
    "        # loops through the cds positions in the fragment\n",
    "        \n",
    "        pos = nt+1 ###important needs to stay like that\n",
    "        # adds the primer region offset specific to the fragment\n",
    "        \n",
    "        if nt % 3 == 0:\n",
    "            codon += 1\n",
    "            # use modulo to delimit and increment codons\n",
    "            \n",
    "        codon_groups[pos] = codon\n",
    "        # add cds position to dict of codon groups\n",
    "        \n",
    "        variant_abundance_dict[codon] = {}\n",
    "        # make an empty container of codon abundances for this specific codon\n",
    "        \n",
    "    wt_codons = {}\n",
    "    # container for the WT codons of the fragment\n",
    "        \n",
    "    ref = ref_seq\n",
    "    # reference dna sequence\n",
    "    \n",
    "    for aa in range(0, n_aa):\n",
    "        # for each amino acid in the fragment\n",
    "        \n",
    "        offset = 0\n",
    "        start = offset+(aa*3)\n",
    "        # define the starting position of the codon in the DNA sequence based on \n",
    "        # aa position and offset\n",
    "        \n",
    "        wt_codon=ref[start:(start+3)]  \n",
    "\n",
    "        wt_codons[(aa+codon_start)] = wt_codon\n",
    "        # extract WT codons from sequence and add it to the dict\n",
    "        \n",
    "        variant_abundance_dict[aa+codon_start][wt_codon]=np.nan \n",
    "        # set abundance to nan for wt codons at each positions (the WT sequence\n",
    "        # is the same across all positions, so impossible to distinguish)          \n",
    "    \n",
    "    for variant in variants:\n",
    "        # loop through detected sequence variants\n",
    "        \n",
    "        var_info = variant.split(',')\n",
    "        var_count =int(var_info[1].split(';')[1].strip('size='))\n",
    "        # extract the number of counts\n",
    "               \n",
    "        mut_list = mutation_set[variant][0]\n",
    "        # get the list of differences between the sequence and the reference\n",
    "        \n",
    "        filtered_list = []\n",
    "        # list of mutations after filtering\n",
    "        \n",
    "        if var_count>=20:\n",
    "            # check variant abundance against arbitrary threshold. For QC purpose, to see what bias\n",
    "            # might exist in low abundance sequences\n",
    "        \n",
    "            for mutation in mut_list:\n",
    "                if 'del' in mutation:\n",
    "                    # loops through mutations to find deletions\n",
    "                    \n",
    "                    mut_info = mutation.split('del')\n",
    "                    mut_pos = int(mut_info[0])\n",
    "                    # extract deletion position\n",
    "                    \n",
    "                    if mut_pos == 1:\n",
    "                        mut_list.remove(mutation)\n",
    "                        # if the insertion is at the start (primer region),\n",
    "                        # ignore it\n",
    "                        \n",
    "            if len(mut_list) <=3 and 'ins' not in str(mut_list) and 'del' not in str(mut_list):\n",
    "                # filter variants: no indels, and not more than three SNVs\n",
    "                \n",
    "                if len(mut_list) ==0:\n",
    "                    wt_count += var_count\n",
    "                    # if not mutations are found, add counts to the WT genotype\n",
    "                    \n",
    "                else:\n",
    "                    mut_nt_list = []\n",
    "                    out_list = []\n",
    "                    # containers for the codons where mutations in the cds occured\n",
    "                    # and those in the primer regions\n",
    "                    \n",
    "                    for mutation in mut_list:\n",
    "                        mut_pos = int(mutation.split('|')[1])\n",
    "                        # loop through mutations and extract position in the sequence\n",
    "                        \n",
    "                        if mut_pos >=1 and mut_pos <= wt_len:\n",
    "                            mut_nt_list.append(codon_groups[mut_pos])\n",
    "                            # if mutation is within the cds, add the mutated codon\n",
    "                            \n",
    "                        else:\n",
    "                            out_list.append(mut_pos)\n",
    "                            # if outside, add to the other list\n",
    "                        \n",
    "                    if len(set(mut_nt_list)) == 1:\n",
    "                        # checks if all the mutations that occured are in the same codon\n",
    "                        \n",
    "                        valid_seq+=var_count\n",
    "                        # if it passes the filter, add the varinat count to the total of valid \n",
    "                        # counts\n",
    "                        \n",
    "                        codon = int(list(set(mut_nt_list))[0])\n",
    "                        # extract the codon where the mutation occured\n",
    "                        \n",
    "                        wt_seq = wt_codons[codon]\n",
    "                        # get the WT at the position where the mutation occured\n",
    "                                                                      \n",
    "                        new_seq = [x for x in wt_seq]\n",
    "                        # stores the wt codon in a new object that we will be able to modify\n",
    "                        # to change the codon\n",
    "                        \n",
    "                        for mutation in mut_list:\n",
    "                            mut_pos = int(mutation.split('|')[1])\n",
    "                            mutation = mutation.split('|')[2]\n",
    "                            # loops through the mutations the variant bears, and extract\n",
    "                            # the changes and their positions\n",
    "                            \n",
    "                            codon_pos = (mut_pos-1)%3\n",
    "                            # convert cds position to position within \n",
    "                            \n",
    "                            new_seq[codon_pos] = mutation\n",
    "                            # makes the change in the mutable codon object\n",
    "                            \n",
    "                        new_codon = ''.join(new_seq)\n",
    "                        # convert codon from list of nt to a string\n",
    "                                                   \n",
    "                        if new_codon in list(variant_abundance_dict[codon].keys()):\n",
    "                            variant_abundance_dict[codon][new_codon]+=var_count\n",
    "                            # if the variant had already been observed before, increment the \n",
    "                            # read count for it\n",
    "                            \n",
    "                        else:\n",
    "                            variant_abundance_dict[codon][new_codon]=var_count\n",
    "                            # if not, create a new entry\n",
    "                            \n",
    "  \n",
    "                        \n",
    "                    elif len(set(mut_nt_list)) == 0 and len(out_list)>=1:\n",
    "                        wt_count+=var_count\n",
    "                        # if no mutations occured in the cds (all in the primer region)\n",
    "                        # increment the WT seq counts\n",
    "                    \n",
    "                    \n",
    "        elif var_count < 20:\n",
    "            # check variant abundance against arbitrary threshold. For QC purpose, to see what bias\n",
    "            # might exist in low abundance sequence\n",
    "\n",
    "            for mutation in mut_list:\n",
    "                if 'del' in mutation:\n",
    "                    # loops through mutations to find deletions\n",
    "                    \n",
    "                    mut_info = mutation.split('del')\n",
    "                    mut_pos = int(mut_info[0])\n",
    "                    # extract deletion position\n",
    "\n",
    "                    if mut_pos == 1:\n",
    "                        mut_list.remove(mutation)\n",
    "                        # if the insertion is at the start (primer region),\n",
    "                        # ignore it\n",
    "                  \n",
    "            if len(mut_list) <=3 and 'ins' not in str(mut_list) and 'del' not in str(mut_list):\n",
    "                # filter variants: no indels, and not more than three SNVs\n",
    "\n",
    "                if len(mut_list) ==0:\n",
    "                    wt_count += var_count\n",
    "                    # if not mutations are found, add counts to the WT genotype\n",
    "\n",
    "                else:\n",
    "                    mut_nt_list = []\n",
    "                    out_list = []\n",
    "                    # containers for the codons where mutations in the cds occured\n",
    "                    # and those in the primer regions\n",
    "\n",
    "                    for mutation in mut_list:\n",
    "                        mut_pos = int(mutation.split('|')[1])\n",
    "                        # loop through mutations and extract position in the sequence\n",
    "\n",
    "                        if mut_pos >=1 and mut_pos <= wt_len:\n",
    "                            mut_nt_list.append(codon_groups[mut_pos])\n",
    "                            # if mutation is within the cds, add the mutated codon\n",
    "\n",
    "                        else:\n",
    "                            out_list.append(mut_pos)\n",
    "                            # if outside, add to the other list\n",
    "\n",
    "                    if len(set(mut_nt_list)) == 1:\n",
    "                        # checks if all the mutations that occured are in the same codon\n",
    "                        \n",
    "                        valid_seq+=var_count\n",
    "                        # if it passes the filter, add the varinat count to the total of valid \n",
    "                        # counts\n",
    "\n",
    "                        codon = int(list(set(mut_nt_list))[0])\n",
    "                        # extract the codon where the mutation occured\n",
    "\n",
    "                        wt_seq = wt_codons[codon]\n",
    "                        # get the WT at the position where the mutation occured\n",
    "\n",
    "                        new_seq = [x for x in wt_seq]\n",
    "                        # stores the wt codon in a new object that we will be able to modify\n",
    "                        # to change the codon\n",
    "\n",
    "                        for mutation in mut_list:\n",
    "                            mut_pos = int(mutation.split('|')[1])\n",
    "                            mutation = mutation.split('|')[2]\n",
    "                            # loops through the mutations the variant bears, and extract\n",
    "                            # the changes and their positions\n",
    "\n",
    "                            codon_pos = (mut_pos-1)%3\n",
    "                            # convert cds position to position within\n",
    "\n",
    "                            new_seq[codon_pos] = mutation\n",
    "                            # makes the change in the mutable codon object\n",
    "\n",
    "                        new_codon = ''.join(new_seq)\n",
    "                        # convert codon from list of nt to a string\n",
    "                        \n",
    "                        if new_codon in list(variant_abundance_dict[codon].keys()):\n",
    "                            variant_abundance_dict[codon][new_codon]+=var_count\n",
    "                            # if the variant had already been observed before, increment the \n",
    "                            # read count for it\n",
    "                            \n",
    "                        else:\n",
    "                            variant_abundance_dict[codon][new_codon]=var_count\n",
    "                            # if not, create a new entry\n",
    "\n",
    "                    \n",
    "    return variant_abundance_dict, wt_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###TADA using every function defined earlier\n",
    "\n",
    "wt_count_dict = {}\n",
    "align_seq_dict = {}\n",
    "\n",
    "for Sample in list(info.index):\n",
    "    \n",
    "    input_seq = needle_folder +str(info.loc[Sample]['Name'])+ '.needle' #path to the needle files\n",
    "    \n",
    "    ##find mutation function, parse_sequence is in it\n",
    "    mut = find_mutations(input_seq, wt_seq) \n",
    "    nbr_aa = (len(wt_aa))\n",
    "\n",
    "    nbr_align = get_n_align(mut)\n",
    "    \n",
    "    ##Get variant count   get_variant_count_1(mutation_set, ref_seq, frag_start, codon_start, n_aa)\n",
    "    print((info.loc[Sample]['Name']))\n",
    "    variants = get_variant_count_1(mut, wt_seq, 0, aa_start,nbr_aa) \n",
    "    exp = str(info.loc[Sample]['Name'])\n",
    "    \n",
    "    df = pd.DataFrame(variants[0]) ##build a dataframe from the variants we got\n",
    "    df = df.reindex(index = codons_all)\n",
    "    name = variants_folder + \"Variants_count_all_\" + exp+\"_\" + date  \n",
    "    df.to_excel(f\"{name}.xlsx\")    ##save it\n",
    "    df.to_csv(f\"{name}.csv\")\n",
    "    \n",
    "    dflog = np.log2(df)\n",
    "    \n",
    "    df2 = df[pos_mutated]\n",
    "    df2 = df2.loc[codons_mutated]\n",
    "    name = variants_folder + \"Variants_count_mut_\" + exp+\"_\" + date  \n",
    "    df2.to_excel(f\"{name}.xlsx\")    ##save it\n",
    "    df2.to_csv(f\"{name}.csv\")\n",
    "    \n",
    "    df2log = np.log2(df2)\n",
    "    \n",
    "    wt_count_dict[str(info.loc[Sample][\"Name\"])] = variants[1] #getting the number of WT\n",
    "    align_seq_dict[str(info.loc[Sample][\"Name\"])] = nbr_align #getting the number of sequences that passed the needle filter\n",
    "    \n",
    "    ##heatmap showing the variants counts \n",
    "    #1\n",
    "    plt.subplots(figsize=(40,15))\n",
    "    title = str(info.loc[Sample]['Name']) + \" - Raw reads count in log2 for all positions\"\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    sns.heatmap(dflog, square=True,cmap = \"viridis\",cbar_kws={'ticks':MaxNLocator(2), 'format':'%.e'},vmin=0).set(title=title)\n",
    "    name = heatmaps_folder + \"Heatmap_variants_all_log_\" + exp+\"_\" + date  \n",
    "    plt.savefig(f\"{name}.png\", format='png', dpi=300) #save it\n",
    "    \n",
    "    #2\n",
    "    plt.subplots(figsize=(40,15))\n",
    "    title = str(info.loc[Sample]['Name']) + \" - Raw reads count for all positions\"\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    sns.heatmap(df, square=True,cmap = \"viridis\",vmin=0).set(title=title)\n",
    "    name = heatmaps_folder + \"Heatmap_variants_all_\" + exp+\"_\" + date  \n",
    "    plt.savefig(f\"{name}.png\", format='png', dpi=300) #save it\n",
    "    \n",
    "    #3\n",
    "    plt.subplots(figsize=(20,20))\n",
    "    title = str(info.loc[Sample]['Name']) + \" - Raw reads count in log2 for library positions only\"\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    sns.heatmap(df2log, square=True,cmap = \"viridis\",cbar_kws={'ticks':MaxNLocator(2), 'format':'%.e'},vmin=0).set(title=title)\n",
    "    name = heatmaps_folder + \"Heatmap_variants_mut_log_\" + exp +\"_\" + date  \n",
    "    plt.savefig(f\"{name}.png\", format='png', dpi=300) #save it   \n",
    "    \n",
    "    #4\n",
    "    plt.subplots(figsize=(20,20)) \n",
    "    title = str(info.loc[Sample]['Name'])+ \" - Raw reads count for library positions only\"\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    sns.heatmap(df2, square=True,cmap = \"viridis\",vmin=0).set(title=title)\n",
    "    name = heatmaps_folder + \"Heatmap_variants_mut_\" + exp+\"_\" + date  \n",
    "    plt.savefig(f\"{name}.png\", format='png', dpi=300) #save it \n",
    "    \n",
    "    #5\n",
    "    df2_freq = df2/nbr_align\n",
    "    df2_freq_log = np.log2(df2_freq +1)\n",
    "    name = variants_folder + \"Variants_count_mut_normalised\" + exp+\"_\" + date  \n",
    "    df2_freq.to_excel(f\"{name}.xlsx\")    ##save it\n",
    "    df2_freq.to_csv(f\"{name}.csv\")\n",
    "    \n",
    "    plt.subplots(figsize=(20,20))\n",
    "    title = str(info.loc[Sample]['Name'])+ \"- Library positions reads normalized with total reads count in log2\"\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    sns.heatmap(df2_freq_log, square=True,cmap = \"viridis\",cbar_kws={'ticks':MaxNLocator(2), 'format':'%.e'}).set(title=title)\n",
    "    name = heatmaps_folder + \"Heatmap_Freq_DMS_variants_log_\" + exp +\"_\" + date  \n",
    "    plt.savefig(f\"{name}.png\", format='png', dpi=300) #save it  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_align = pd.DataFrame.from_dict(after_align_dict, orient=\"index\", columns = [\"reads_align\"])\n",
    "\n",
    "wt_count = pd.DataFrame.from_dict(wt_count_dict, orient=\"index\", columns = [\"reads_wt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = pd.concat([reads, reads_align], axis=1)\n",
    "reads = pd.concat([reads, wt_count], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##concat all the reads counts\n",
    "\n",
    "reads['percent_after_align'] = (reads['reads_align'] / reads['reads_after_merge'])*100\n",
    "reads['percent_WT_seq_align'] = (reads['reads_wt'] / reads['reads_align'])*100\n",
    "reads['percent_WT_seq_after_merge'] = (reads['reads_wt'] / reads['reads_after_merge'])*100\n",
    "\n",
    "name = reads_counts_folder + \"Reads_count_\" + experiment +\"_\" + date  \n",
    "reads.to_excel(f\"{name}.xlsx\")    ##save it\n",
    "reads.to_csv(f\"{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##supperpose total reads and WT\n",
    "\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=info[\"Name\"], height=reads_align['reads_align'], width=0.35,align='center')\n",
    "ax.bar(x=info[\"Name\"], height=wt_count['reads_wt'], width=0.35/2,  align='center')\n",
    "plt.yscale(\"log\", base=2)\n",
    "plt.title(\"Total number of aligned reads and WT\", size=12)\n",
    "plt.xticks(rotation=90, size =8)\n",
    "plt.ylabel('Number of reads in log2', size=12)\n",
    "\n",
    "name = figures_folder + \"Proportion_WT_\" + experiment+\"_\" + date  \n",
    "plt.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##supperpose merge total reads and aligned\n",
    "\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=info[\"Name\"], height=reads['reads_after_merge'], width=0.35,align='center')\n",
    "ax.bar(x=info[\"Name\"], height=reads_align['reads_align'], width=0.35/2,  align='center')\n",
    "plt.yscale(\"log\", base=2)\n",
    "plt.title(\"Total number of merged and aligned reads\", size=12)\n",
    "plt.xticks(rotation=90, size =8)\n",
    "plt.ylabel('Number of reads in log2', size=12)\n",
    "\n",
    "name = figures_folder + \"Merge_Aligned_\" + experiment+\"_\" + date  \n",
    "plt.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
