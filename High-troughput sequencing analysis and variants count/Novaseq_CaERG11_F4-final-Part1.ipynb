{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informations\n",
    "\n",
    "To use this script, you have to start with demultiplexed data containing RC primers (TileSeq in my case). The WT sequence has to be the same for every sample.\n",
    "In theory, you don't have to change the directories if you keep the same folder names.\n",
    "It's important to have a good sample_sheet.xlsx to start with, it will make the analysis much easier especially if you keep the same headers than me.\n",
    "I made the script so you don't have to change anything exept in the set up section.\n",
    "\n",
    "## This is the part 1 of the analysis. Variants count is done in part 2.\n",
    "\n",
    "### Analysis pipeline : \n",
    "### 1- Quality control with fastQC on .fastq.gz files.\n",
    "### 2- From demultiplexed .fastq.gz files, pandaseq will merge the forward and reverse reads.\n",
    "### 3- From .fasta files, vsearch will remove the remaining primers.\n",
    "### 4- From .fasta files, vsearch will aggregate the reads. \n",
    "### 5- From .fasta files, needle will align your reads to the wt sequence.\n",
    "### 6- From .needle files, you will count the number of variants for every codons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'm running everything on Ubuntu wsl for windows downloaded from the appstore. I downloaded Anaconda. It can also be used with the server.\n",
    "\n",
    "### 1- FastQc = https://anaconda.org/bioconda/fastqc\n",
    "### 2- PandaSeq = https://anaconda.org/bioconda/pandaseq\n",
    "### 3- Vsearch = https://anaconda.org/bioconda/vsearch\n",
    "### 4- Needle from Emboss = https://anaconda.org/bioconda/emboss\n",
    "\n",
    "## Quality controls are done along the way with graphs and dataframes to know the quantity of reads we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import Bio\n",
    "import subprocess\n",
    "import openpyxl\n",
    "from Bio import SeqIO\n",
    "import os,sys,re\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.__name__, pd.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__name__, np.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "print(matplotlib.__name__, matplotlib.__version__)\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "print(scipy.__name__, scipy.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "print(sns.__name__, sns.__version__)\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "Ideally this is the only cell which has to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##see the sample sheet to know how to fill it\n",
    "##info = dataframe of the sample_sheet\n",
    "## wt = the path to the wt sequence in a .fa file\n",
    "## wt_seq = wt sequence directly in a string\n",
    "## *the wt sequence has to be without the primers, so how it will be once trimmed.\n",
    "\n",
    "if not os.path.exists('CaERG11-F4'):\n",
    "    os.makedirs('CaERG11-F4')\n",
    "\n",
    "if not os.path.exists('CaERG11-F4-merged_reads'):\n",
    "    os.makedirs('CaERG11-F4-merged_reads')\n",
    "    \n",
    "if not os.path.exists('CaERG11-F4-figures'):\n",
    "    os.makedirs('CaERG11-F4-figures')\n",
    "\n",
    "if not os.path.exists('CaERG11-F4-Trimmed'):\n",
    "    os.makedirs('CaERG11-F4-Trimmed')\n",
    "\n",
    "if not os.path.exists('CaERG11-F4-Aggregated'):\n",
    "     os.makedirs('CaERG11-F4-Aggregated')\n",
    "\n",
    "if not os.path.exists('CaERG11-F4-variants'):\n",
    "    os.makedirs('CaERG11-F4-variants')\n",
    "    \n",
    "if not os.path.exists('CaERG11-F4-ReadsCount'):\n",
    "    os.makedirs('CaERG11-F4-ReadsCount')\n",
    "    \n",
    "if not os.path.exists('CaERG11-F4-Needle'):\n",
    "    os.makedirs('CaERG11-F4-Needle')\n",
    "    \n",
    "if not os.path.exists('CaERG11-F4-Heatmaps'):\n",
    "    os.makedirs('CaERG11-F4-Heatmaps')\n",
    "\n",
    "\n",
    "date = \"2023-06-20\" #date your doing the analysis, used to save figures\n",
    "experiment = \"CaERG11-F4\" #generic name of your experiment, used to save figures\n",
    "reads_folder = \"./CaERG11-F4/\" #location of the reads in .gz\n",
    "merged_folder = \"./CaERG11-F4-merged_reads/\" # location of merged reads\n",
    "figures_folder = \"./CaERG11-F4-figures/\" # location of the figures\n",
    "trim_folder = \"./CaERG11-F4-Trimmed/\" #location of trimmed reads\n",
    "agg_folder = \"./CaERG11-F4-Aggregated/\" #location of aggregated reads\n",
    "needle_folder = \"./CaERG11-F4-Needle/\" #location of reads aligned to wt (.needle)\n",
    "variants_folder = \"./CaERG11-F4-variants/\" #location of the variants count\n",
    "reads_counts_folder = \"./CaERG11-F4-ReadsCount/\" #location of all dataframes output in excel format with reads counts \n",
    "\n",
    "sample_sheet = \"./sample_sheet_F4_all.xlsx\" #excel sheet with all the information\n",
    "\n",
    "pe = 250 #paired-end\n",
    "\n",
    "before_nut = 1 #number of nucleotides before the first complete codon of your sequence (0, 1 or 2)\n",
    "after_nut = 1 #number of nucleotides after the last complete codon of your sequence (0, 1 or 2)\n",
    "\n",
    "aa_start = 393 #first amino acid of your sequence (complete codon)\n",
    "aa_end = 514 #last amino acid of your sequence (complete codon)\n",
    "\n",
    "wt = \"./CaERG11_wt_seq/CaERG11_F4_wt_seq.fasta\" #your WT sequence AFTER trimming\n",
    "wt_seq = \"ATATTCAGAAAAGTTACTAACCCACTTAGGATCCCTGAAACCAACTACATCGTCCCAAAAGGACACTACGTTCTTGTCAGCCCAGGCTACGCACACACGAGTGAGAGATACTTTGATAACCCGGAGGATTTTGATCCTACACGTTGGGATACTGCTGCAGCCAAAGCCAATTCTGTAAGCTTTAACTCCAGTGATGAGGTAGATTACGGCTTTGGGAAAGTATCAAAAGGCGTCAGCTCACCATATCTTCCCTTCGGTGGCGGTAGACATAGATGTATAGGTGAACAATTTGCATACGTTCAGCTGGGAACCATATTAACGACGTTTGTTTATAACTTGAGATGGACTATCGACGGGTATAAAGTCCCTGATCCTGACTATAGCTCTATGGTTGTTctaCCCACCGAA\"\n",
    "pos_mutated = [401,402,403,404,405,406,407,408,410,432,434,436,446,447,448,449,450,452,456,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,479,480,481,483,484,503,504,505,506,507,508,509,510,511]\n",
    "codons_mutated = [\"GCT\",\"GCC\",\"TGC\",\"TGT\",\"GAT\",\"GAC\",\"GAG\",\"GAA\",\n",
    "\"TTC\",\"TTT\",\"GGT\",\"GGA\",\"CAC\",\"CAT\",\"ATC\",\"ATT\",\"AAG\",\"AAA\",\"TTG\",\"TTA\",\"ATG\",\"AAC\",\"AAT\",\"CCT\",\"CCA\",\"CAG\",\"CAA\",\n",
    "\"AGA\",\"CGT\",\"TCT\",\"TCC\",\"ACC\",\"ACT\",\"GTT\",\"GTC\",\"TGG\",\"TAC\",\"TAT\",\"TAA\"] \n",
    "codons_all = [\"GCT\",\"GCC\",\"TGC\",\"TGT\",\"GAT\",\"GAC\",\"GAG\",\"GAA\",\n",
    "\"TTC\",\"TTT\",\"GGT\",\"GGA\",\"CAC\",\"CAT\",\"ATC\",\"ATT\",\"AAG\",\"AAA\",\"TTG\",\"TTA\",\"ATG\",\"AAC\",\"AAT\",\"CCT\",\"CCA\",\"CAG\",\"CAA\",\n",
    "\"AGA\",\"CGT\",\"TCT\",\"TCC\",\"ACC\",\"ACT\",\"GTT\",\"GTC\",\"TGG\",\"TAC\",\"TAT\",\"TAA\",\"GTA\",\"GCA\",\"GTG\",\"ATA\",\"GCG\",\"CTC\",\"CTA\",\"CGG\",\n",
    "\"TAG\",\"TGA\",\"CCC\",\"GGG\",\"TCG\",\"AGG\",\"CGA\",\"CGC\",\"AGT\",\"CTG\",\"ACG\",\"TCA\",\"AGC\",\"GGC\",\"CTT\",\"ACA\",\"CCG\"]\n",
    "wt_len = len(wt_seq)\n",
    "wt_aa=\"IFRKVTNPLRIPETNYIVPKGHYVLVSPGYAHTSERYFDNPEDFDPTRWDTAAAKANSVSFNSSDEVDYGFGKVSKGVSSPYLPFGGGRHRCIGEQFAYVQLGTILTTFVYNLRWTIDGYKVPDPDYSSMVVLPTE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_excel(sample_sheet, header=0, index_col=0)\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a funtion to get the FastQC\n",
    "\n",
    "def quality_fastqc(input_fastq_file):\n",
    "\n",
    "    subprocess.check_output('fastqc '+input_fastq_file, shell=True)\n",
    "    # Run Fastqc on the compressed sequencing file\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#don't print the output\n",
    "##Let's iterate trough all the sequencing file and use our quality_fastqc function\n",
    " \n",
    "# giving file extension we want\n",
    "ext = ('fastq.gz')\n",
    " \n",
    "# iterating over all files\n",
    "#for files in os.listdir(reads_folder):\n",
    "    #if files.endswith(ext):\n",
    "        #quality_fastqc(reads_folder + files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##find the number of reads in the fastq.gz files\n",
    "#we divide by 4 because there is 4 lines in the file for 1 read\n",
    "\n",
    "\n",
    "before_merge_depth_dict = {}\n",
    "\n",
    "for Sample in list(info.index):\n",
    "    fullpath = reads_folder +info.loc[Sample]['reads_file_For']\n",
    "    print(fullpath)\n",
    "    call =\"gunzip -c \" + fullpath + \" |wc -l\"\n",
    "    result = subprocess.check_output(call, shell=True)\n",
    "    numseqs = int(result)/4.0\n",
    "    before_merge_depth_dict[str(info.loc[Sample][\"Name\"])] = numseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_before = pd.DataFrame.from_dict(before_merge_depth_dict, orient=\"index\", columns = [\"reads_before\"])\n",
    "reads_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at how much read we have before merging\n",
    "#fig, ax = plt.subplots(figsize=(25,15))\n",
    "\n",
    "#ax.set_yscale('log', base=2)\n",
    "\n",
    "#df_size = len(info.index)\n",
    "\n",
    "#plt.bar(info[\"Name\"], reads_before[\"reads_before\"])\n",
    "\n",
    "#plt.title(\"Reads before merging\")\n",
    "#plt.xlabel('Samples', fontsize=14)\n",
    "#plt.ylabel('Number of reads in log2', fontsize=14)\n",
    "#plt.xticks(rotation=90)\n",
    "\n",
    "#name = figures_folder + \"Reads_Before_\" + experiment + date  \n",
    "#plt.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge reads with Pandseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_reads(Sample):\n",
    "    # this function generates and runs a Pandaseq call to merge R1 and R2 for each demultiplexed sample.\n",
    "    # it returns the starting number of reads in the file, which is also stored in a dict\n",
    "\n",
    "    filepath_for = reads_folder + info.loc[Sample]['reads_file_For']\n",
    "    filepath_rev = reads_folder + info.loc[Sample]['reads_file_Rev']\n",
    "    \n",
    "                \n",
    "    filepath_out = merged_folder+str(info.loc[Sample]['Name'])+'.fasta'\n",
    "        # define the merging output filepath\n",
    "        \n",
    "    for_len = len(info.loc[Sample]['RC_for_seq'])\n",
    "    rev_len = len(info.loc[Sample]['RC_rev_seq'])\n",
    "    wt_len = len(wt_seq)\n",
    "        \n",
    "        #size = wt_len+for_len+rev_len\n",
    "        #maxi = str(size + 50)\n",
    "        #mini = str(size - 50)\n",
    "        #overlap = ((size-pe) - pe)*-1\n",
    "        #over_max = str(overlap +25)\n",
    "        #over_min = str(overlap -25)\n",
    "        #size = str(size)        \n",
    "        #panda_seq_call = 'pandaseq -f '+filepath_for+' -r '+filepath_rev+\" -L \" + maxi +\" -l \" + mini +\" -O \" + over_max +\" -o \"+ over_min+\" -k + 4 -B -N -t 0.5 -T 6 -w \"+filepath_out\n",
    "        #print(panda_seq_call)\n",
    "        \n",
    "    print(\"LOL\")\n",
    "    panda_seq_call = 'pandaseq -f '+filepath_for+' -r '+filepath_rev+' -L 500 -l 350 -O 100 -o 70 -k 4 -B -N -t 0.5 -T 6 -w '+ filepath_out\n",
    "    subprocess.check_output(panda_seq_call, shell=True)\n",
    "        # generates and runs a pandseq call with the following parameters:\n",
    "        #    -L              maximum length of assembled sequence: 550 pb\n",
    "        #    -O              maximum overlap between reads: 400 pb\n",
    "        #    -k             number of sequence locations per kmer (increased from default value of 2)\n",
    "        #    -B                 allow input to lack a barcode\n",
    "        #    -N                 eliminate all sequences with Ns\n",
    "        #    -t 0.5             score threshold\n",
    "        #    -T 6               use 6 threads\n",
    "        #    -w filepath_out    write output to filepath_out\n",
    "        \n",
    "    return filepath_out\n",
    "    # return initial number of reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for Sample in list(info.index):\n",
    "    # for sample in DataFrame\n",
    "    merge_reads(Sample)\n",
    "    # merge reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_merge_depth_dict = {}\n",
    "after_merge_depth_list = []\n",
    "# empty container that wil hold the post-merge number of reads for each sample\n",
    "\n",
    "def count_merged(Sample):\n",
    "    \n",
    "    filepath = merged_folder\n",
    "    filepath_merged = str(info.loc[Sample]['Name'])+'.fasta'\n",
    "    filepath+=filepath_merged\n",
    "    print(filepath)\n",
    "    # find merged reads fasta file path based on sample name\n",
    "    \n",
    "    after_merge_depth = 0\n",
    "    # counter for read number\n",
    "      \n",
    "    with open(filepath, 'r') as source:\n",
    "        for line in source:\n",
    "            if line.startswith('>'):\n",
    "                after_merge_depth += 1\n",
    "                # open merged read file, loop through lines,\n",
    "                # find the headers and increment by one for each header \n",
    "\n",
    "    after_merge_depth_dict[str(info.loc[Sample][\"Name\"])] = after_merge_depth\n",
    "    after_merge_depth_list.append(after_merge_depth)\n",
    "    # once file has been read through, add read number to the dictionary\n",
    "    return after_merge_depth\n",
    "    # return merged read counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Sample in list(info.index):\n",
    "    # for each sample\n",
    "    count_merged(Sample)\n",
    "    # count the number of reads after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_merge_depth_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_after_merge = pd.DataFrame.from_dict(after_merge_depth_dict, orient=\"index\", columns = [\"reads_after_merge\"])\n",
    "reads_after_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at how much read we got after merging\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(25,15))\n",
    "\n",
    "#ax.set_yscale('log', base=2)\n",
    "\n",
    "#df_size = len(info.index)\n",
    "#plt.bar(info[\"Name\"], reads_after[\"reads_after\"])\n",
    "\n",
    "#plt.title(\"Reads after merging\")\n",
    "#plt.xlabel('Samples', fontsize=14)\n",
    "#plt.ylabel('Number of reads in log2', fontsize=14)\n",
    "#plt.xticks(rotation=90)\n",
    "\n",
    "#name = figures_folder + \"Reads_After_Merge_\" + experiment+\"_\" + date  \n",
    "#plt.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##supperpose before and after merging number of reads\n",
    "\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=info[\"Name\"], height=reads_before[\"reads_before\"], width=0.35,align='center')\n",
    "ax.bar(x=info[\"Name\"], height=reads_after_merge[\"reads_after_merge\"], width=0.35/2,  align='center')\n",
    "plt.yscale(\"log\", base=2)\n",
    "plt.title(\"Number of reads before and after merging\")\n",
    "plt.xticks(rotation=90, size =8)\n",
    "plt.ylabel('Number of reads in log2', size=12)\n",
    "\n",
    "name = figures_folder + \"Reads_Before_After_Merge_\" + experiment+\"_\" + date  \n",
    "plt.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim and aggregate reads with vsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_aggregate(Sample):\n",
    "    # This function generates and runs two vsearch calls to trim and then aggregate the merged reads.\n",
    "    # Trimming removes the variable degenreate sequences, which then  allows us to aggregate the sequences that have\n",
    "    # the same sequence with minimal influence from amplicons regions used for multiplexing.\n",
    "       \n",
    "    \n",
    "    filepath_merged = merged_folder+str(info.loc[Sample]['Name'])+'.fasta'\n",
    "    # get the merged read filepath based on the sample\n",
    "    \n",
    "    trim_path = trim_folder + str(info.loc[Sample]['Name'])+ '.fasta'\n",
    "    aggregate_path = agg_folder +str(info.loc[Sample]['Name'])+ '.fasta'\n",
    "    # generate output filepaths for the trimming and aggregating steps\n",
    "    \n",
    "    #we need the length of the primers to cut them\n",
    "    for_len = str(len(info.loc[Sample]['RC_for_seq'])+before_nut)\n",
    "    rev_len = str(len(info.loc[Sample]['RC_rev_seq'])+after_nut)\n",
    "    \n",
    "    vsearch_trim_call = 'vsearch --fastx_filter '+filepath_merged+\" --fastq_stripleft \"+ for_len+ \" --fastq_stripright \"+ rev_len+\" --fastaout \" + trim_path\n",
    "    subprocess.check_output(vsearch_trim_call, shell=True)\n",
    "    # generates and runs a vsearch call with the following parameters:\n",
    "    #    --fastx_filter filepath_merged    vsearch program to be used and input: fastxfilter can perform different\n",
    "    #                                      operations on fastq files\n",
    "    #    --fastq_stripleft                 removes 76 bases from the 5p end\n",
    "    #    --fastq_stripright 76             removes 76 bases from the 3p end\n",
    "    #    --fastaout trim_path              ouput trimmed sequences the trim_path file                 \n",
    "    \n",
    "    vsearch_aggregate_call = 'vsearch --derep_fulllength '+ trim_path +' --relabel seq --output '+aggregate_path+' --sizeout'\n",
    "    subprocess.check_output(vsearch_aggregate_call, shell=True)\n",
    "    # generates and runs a vsearch call with the following parameters:\n",
    "    #    --derep_fulllength trim_path    vsearch program to be used and input. derep_fulllength aggregates perfectly identical \n",
    "    #                                    sequences in the input fasta, dramatically reducing the number of sequences that then \n",
    "    #                                    need to be aligned\n",
    "    #    --relabel seq                   changes header so that all sequence names begin by seq\n",
    "    #    --output                        path where output will be written\n",
    "    #    --sizeout                       append the size of the sequence cluster to the fasta header\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "for Sample in list(info.index):\n",
    "    \n",
    "    trim_aggregate(Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Get unique/different and singleton sequences in aggregated reads\n",
    "\n",
    "after_agg_unique_dict = {}\n",
    "after_agg_single_dict = {}\n",
    "after_agg_two_dict = {}\n",
    "\n",
    "for Sample in list(info.index):\n",
    "    #Find the aggregated fasta file\n",
    "    file_to_count=agg_folder + str(info.loc[Sample]['Name'])+'.fasta'\n",
    "    #write bash command to count all unique reads per library\n",
    "    cmdline_unique=f'grep -c \">\" {file_to_count}'\n",
    "    #Run bash line to count unique reads\n",
    "    unique_seq_number=subprocess.getoutput(cmdline_unique)\n",
    "    #Write number of unique sequences to dict\n",
    "    after_agg_unique_dict[str(info.loc[Sample][\"Name\"])] = int(unique_seq_number)\n",
    "    #Write bash line to count all sequences with one read (singletons)\n",
    "    cmdline_singleton = f'grep -c \"size=1\" {file_to_count}'\n",
    "    #Run bash line to count singletons\n",
    "    singleton_number=subprocess.check_output(cmdline_singleton, shell=True)\n",
    "    #Write number of singleton sequences to dict\n",
    "    after_agg_single_dict[str(info.loc[Sample][\"Name\"])] = int(singleton_number)\n",
    "    #Write bash line to count all sequences with 2 reads (almost singletons)\n",
    "    cmdline_two = f'grep -c \"size=2\" {file_to_count}'\n",
    "    #Run bash line to count singletons\n",
    "    two_number=subprocess.check_output(cmdline_two, shell=True)\n",
    "    #Write number of singleton sequences to df\n",
    "    after_agg_two_dict[str(info.loc[Sample][\"Name\"])] = int(two_number)\n",
    "\n",
    "del(file_to_count, cmdline_unique, unique_seq_number, cmdline_singleton, singleton_number, Sample )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_agg = pd.DataFrame.from_dict(after_agg_unique_dict, orient=\"index\", columns = [\"nbr_seq_unique\"])\n",
    "seq_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_agg_single = pd.DataFrame.from_dict(after_agg_single_dict, orient=\"index\", columns = [\"nbr_seq_single\"])\n",
    "seq_agg_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_agg_two = pd.DataFrame.from_dict(after_agg_two_dict, orient=\"index\", columns = [\"nbr_seq_two\"])\n",
    "seq_agg_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's look at how much different reads we got after aggregate\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25,15))\n",
    "\n",
    "ax.set_yscale('log', base=2)\n",
    "\n",
    "df_size = len(info.index)\n",
    "plt.bar(info[\"Name\"], seq_agg[\"nbr_seq_unique\"])\n",
    "\n",
    "plt.title(\"Different Reads after aggregate\")\n",
    "plt.xlabel('Samples', fontsize=14)\n",
    "plt.ylabel('Number of reads in log2', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "name = figures_folder + \"Reads_After_Aggregate_Unique_\" + experiment+\"_\" + date  \n",
    "plt.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's look at how much singleton reads we got after aggregate\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25,15))\n",
    "\n",
    "ax.set_yscale('log', base=2)\n",
    "\n",
    "df_size = len(info.index)\n",
    "plt.bar(info[\"Name\"], seq_agg_single[\"nbr_seq_single\"])\n",
    "\n",
    "plt.title(\"Singleton reads after aggregate\")\n",
    "plt.xlabel('Samples', fontsize=14)\n",
    "plt.ylabel('Number of reads in log2', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "name = figures_folder + \"Reads_After_Aggregate_Single_\" + experiment +\"_\"+ date  \n",
    "plt.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the size distribution\n",
    "\n",
    "\n",
    "\n",
    "nbr_reads_single = {}\n",
    "\n",
    "for Sample in list(info.index):\n",
    "    file_to_count=agg_folder + str(info.loc[Sample]['Name'])+'.fasta'\n",
    "    \n",
    "    with open(file_to_count, 'r') as source:\n",
    "        \n",
    "\n",
    "        exp = str(info.loc[Sample]['Name'])\n",
    "        print(exp)\n",
    "        \n",
    "        singleton=0\n",
    "        other=0\n",
    "        \n",
    "        for line in source:\n",
    "            if \"size=1\" in line:\n",
    "                singleton += 1\n",
    "            else :\n",
    "                other += 1\n",
    "                \n",
    "        nbr_reads_single[str(info.loc[Sample][\"Name\"])] = int(singleton)\n",
    "\n",
    "        print(str(singleton))\n",
    "        print(str(other))\n",
    "\n",
    "        #sns.set(style=\"white\")\n",
    "        #plt = sns.histplot(data=size, x=size, color = \"green\", bins = 10, binwidth=0.01)\n",
    "        #plt.set_yscale(\"log\", base=2)\n",
    "        #title = str(info.loc[Sample]['Name'])\n",
    "        #plt.set_xlabel(\"Occurence of unique aggregated sequence size\", fontsize = 10)\n",
    "        #plt.set_title(title)\n",
    "        #plt.set_ylabel(\"Log Count\", fontsize = 10)\n",
    "\n",
    "        #plt.tick_params(axis='x', rotation=90)\n",
    "        #fig = plt.figure\n",
    "        #fig\n",
    "        #name = figures_folder + \"Number_size_Aggregated_Log_\" + exp +\"_\"+ date  \n",
    "        #fig.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_reads_single = pd.DataFrame.from_dict(nbr_reads_single, orient=\"index\", columns = [\"nbr_reads_single\"])\n",
    "nbr_reads_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##supperpose before and after merging number of reads\n",
    "\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=info[\"Name\"], height=reads_after_merge[\"reads_after_merge\"], width=0.35,align='center')\n",
    "ax.bar(x=info[\"Name\"], height=nbr_reads_single[\"nbr_reads_single\"], width=0.35/2,  align='center')\n",
    "plt.yscale(\"log\", base=2)\n",
    "plt.title(\"Total number of reads and singletons\",size=12)\n",
    "plt.xticks(rotation=90, size =8)\n",
    "plt.ylabel('Number of reads in log2', size=12)\n",
    "\n",
    "name = figures_folder + \"Reads_Total_and_Singletons_\" + experiment+\"_\" + date  \n",
    "plt.savefig(f\"{name}.png\", format='png', dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's do a dataframe with all the reads informations\n",
    "\n",
    "nbr_reads_dict = {}\n",
    "\n",
    "for Sample in list(info.index):\n",
    "\n",
    "    nbr_reads_dict[str(info.loc[Sample][\"Name\"])] = info.loc[Sample][\"nbr_reads\"]\n",
    "\n",
    "nbr_reads_dict\n",
    "reads_wanted = pd.DataFrame.from_dict(nbr_reads_dict, orient=\"index\", columns = [\"reads_wanted\"])\n",
    "df1 = pd.concat([reads_wanted, reads_before], axis=1)\n",
    "df1['reads_percent'] = (df1['reads_before'] / df1['reads_wanted'])*100\n",
    "df1 = pd.concat([df1, reads_after_merge], axis=1)\n",
    "df1['merge_percent'] = (df1['reads_after_merge'] / df1['reads_before'])*100\n",
    "df1 = pd.concat([df1, seq_agg], axis=1)\n",
    "df1 = pd.concat([df1, seq_agg_single], axis=1)\n",
    "df1['nbr_single_seq_percent'] = (df1['nbr_seq_single'] / df1['nbr_seq_unique'])*100\n",
    "\n",
    "df1 = pd.concat([df1, nbr_reads_single], axis=1)\n",
    "df1['percent_singletons'] = (df1['nbr_reads_single'] / df1['reads_after_merge'])*100\n",
    "df1\n",
    "name = reads_counts_folder + \"Reads_count_\" + experiment+\"_\" + date  \n",
    "df1.to_excel(f\"{name}.xlsx\")\n",
    "df1.to_csv(f\"{name}.csv\") ##save it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align to WT sequence with needle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "for Sample in list(info.index):\n",
    "    \n",
    "    #Define file paths\n",
    "    file_in= agg_folder +str(info.loc[Sample]['Name'])+ '.fasta'\n",
    "    file_out= needle_folder +str(info.loc[Sample]['Name'])+ '.needle'\n",
    "    \n",
    "    #Write command line to run the needle alignment tool\n",
    "    needle_call = 'needle -auto -gapopen 100 -asequence '+ wt + ' -bsequence '+ file_in +' -aformat3 markx10 -outfile '+file_out   \n",
    "    subprocess.check_output(needle_call, shell = True)\n",
    "\n",
    "    # generate and run a needle call with the following parameters:\n",
    "    #    -auto                        do not run in interactive mode\n",
    "    #    -gapopen 50                  increase penalty for gap in the alignment. This helps\n",
    "    #                                 with instances where there are no indels in the read but\n",
    "    #                                 needle still opens a gap \n",
    "    #    -asequence ref_fasta_path    reference sequence fasta file\n",
    "    #    -bsequence filepath          query sequences in fasta format\n",
    "    #    -aformat3 markx10            choose the ouput format\n",
    "    #    -ourfile needle_out          define output file path"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
